{"mxGraphModel": {"root": {"mxCell": [{"_id": "0"}, {"Object": {"Object": {"_gpu_id": "0", "_memory_size": "8.0", "_number_of_cpus": "2.0", "_as": "hw_resources"}, "_as": "options"}, "_id": "1", "_parent": "0", "_regions": [{"id": "auto", "name": "app.yonohub.com", "selected": true}], "_hw_resources": {"recommended": {"cpu": 0, "gpu": 0, "memory": 0}, "total": {"cpu": 0, "gpu": 0, "memory": 0}}, "_pipeline_region": "{\"dep_reg_id\":\"auto\",\"node_id\":null,\"burstability\":true,\"selectedResource\":\"\",\"resources_ids\":[],\"resourcesObj\":[],\"usage_factor\":0.1,\"count\":\"1\",\"isBurstableConfigurable\":false,\"spinner\":\"sent\",\"instanceType\":\"C\",\"instanceCount\":20,\"dep_reg_resources\":null,\"isCustomerSite\":false}"}, {"mxGeometry": {"_x": "518", "_y": "645", "_width": "222", "_height": "172", "_as": "geometry"}, "Object": {"Object": {"_gpu_id": "1", "_memory_size": "6.1", "_number_of_cpus": "0.6", "_as": "hw_resources"}, "_batch_size": "1", "_dataset": "7", "_frame_rate": "50", "_loop": "true", "_play": "", "_shuffle": "true", "_split": "test", "_number_of_instances": "1", "_region": "{\"dep_reg_id\":\"auto\",\"node_id\":null,\"burstability\":true,\"selectedResource\":\"\",\"resources_ids\":[],\"resourcesObj\":[],\"usage_factor\":0.1,\"count\":\"1\",\"isBurstableConfigurable\":false,\"spinner\":\"sent\",\"instanceType\":\"GK\",\"instanceCount\":1,\"dep_reg_resources\":{},\"isCustomerSite\":false}", "_execution_mode": "async", "_is_synced": "1", "_synced_values": "{\"batch_size\":\"batch_size\"}", "_as": "options"}, "_id": "2", "_value": "", "_style": "top;fillColor=#fff;strokeColor=#929292;rotatable=0;resizable=0;", "_vertex": "1", "_connectable": "0", "_blockID": "5ef1dcc83edd5aa60ad8fa2a", "_originalBlockID": "5ed4c7576b86ba39478836a4", "_tooltip": "The block is used to stream all the datasets available under the Image Classification category offered by Tensorflow Datasets package in a batch mode to be used in training/ testing of models.", "_block": "1", "_yonoarc_block": "1", "_parent": "1", "_suggested": "1"}, {"mxGeometry": {"_x": "10", "_y": "15", "_width": "200", "_height": "55", "_as": "geometry"}, "_id": "3", "_value": "Image Classifica..", "_style": "top;strokeColor=none;verticalLabelPosition=bottom;labelBackgroundColor=;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;verticalAlign=top;image=https://app.yonohub.com/editor/block-icon/ff3d205fa5c8426db4d6890206da222e.svg;shape=image;", "_vertex": "1", "_tooltip": "Image Classification Batch Player: The block is used to stream all the datasets available under the Image Classification category offered by Tensorflow Datasets package in a batch mode to be used in training/ testing of models.", "_connectable": "0", "_parent": "2"}, {"mxGeometry": {"_y": "137", "_width": "222", "_height": "34", "_as": "geometry"}, "_id": "4", "_value": "", "_style": "shape=stencil(rVNNE4IgFPw13AkudWysTv2Jp2Iyohio2b8PQZvBr2xqhsvuvn1vGR6IBjqFkiGCC8gZoidESMOU5rK4QsiEgUYDXbKo6lVQHELBnJI6coexww8P0zPCR8N2hwYhRNlNybqIPWGQS6jSGWGQc9l0MVs3oB/3dIhSCy8rdsGLRTv+0v2+7Z/8G+Lfa4i7BrtxhzHREkccSF9A/ILtKfdexi3+UchPEbGf8MdH3GCPhNRsvoQGhp9soGUX1pYGiVRsZZ8TLoSulMwmI23biduy7jda4gU=);whiteSpace=wrap;html=1;strokeColor=#929292;fontSize=10;fillColor=#929292;fontColor=white;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_tooltip": "", "_status": "stopped", "_type": "version", "_parent": "2"}, {"mxGeometry": {"_x": "30", "_width": "40", "_height": "34", "_as": "geometry"}, "_id": "5", "_value": "<span class=\"icon-gpu\"></span>", "_style": "html=1;strokeColor=none;fontSize=16;fillColor=transparent;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;fontColor=#FFF;", "_vertex": "1", "_connectable": "0", "_parent": "4"}, {"mxGeometry": {"_x": "70", "_width": "40", "_height": "34", "_as": "geometry"}, "_id": "6", "_value": "<span class=\"icon-training\"></span>", "_style": "html=1;strokeColor=none;fontSize=16;fillColor=transparent;fontColor=#777;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_parent": "4"}, {"mxGeometry": {"_x": "110", "_width": "40", "_height": "34", "_as": "geometry"}, "_id": "7", "_value": "<span class=\"icon-error\"></span>", "_style": "html=1;strokeColor=none;fontSize=16;fillColor=transparent;fontColor=#777;movable=0;resizable=1;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_parent": "4"}, {"mxGeometry": {"_x": "150", "_width": "40", "_height": "34", "_as": "geometry"}, "_id": "8", "_value": "<span class=\"icon-duplication\"></span>", "_style": "html=1;strokeColor=none;fontSize=16;fillColor=transparent;fontColor=#777;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_parent": "4"}, {"mxGeometry": {"_x": "20", "_y": "110", "_width": "180", "_height": "20", "_as": "geometry"}, "_id": "9", "_value": "Yonohub Team", "_style": "logo;fontSize=12;strokeColor=#70707030;strokeWidth=2;arcSize=15;rounded=1;fillColor=none;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_parent": "2"}, {"mxGeometry": {"_x": "0.985", "_y": "0.27", "_width": "6", "_height": "6", "_relative": "1", "_as": "geometry"}, "_id": "10", "_value": "<op></op>", "_style": "rounded=1;arcSize=50;html=1;movable=0;resizable=0;rotatable=0;deletable=0;editable=0;selectable=0;strokeColor=#2d2a53;", "_vertex": "1", "_slug": "image_batches", "_key": "image_batches", "_tooltip": "Image Batch (perception_msgs/ImageBatch)\nA batch of images of the selected dataset", "_message_tag": "perception_msgs/ImageBatch", "_ifFastRtps": "1", "_parent": "2"}, {"mxGeometry": {"_x": "0.985", "_y": "0.5700000000000001", "_width": "6", "_height": "6", "_relative": "1", "_as": "geometry"}, "_id": "11", "_value": "<op></op>", "_style": "rounded=1;arcSize=50;html=1;movable=0;resizable=0;rotatable=0;deletable=0;editable=0;selectable=0;strokeColor=#2d2a53;", "_vertex": "1", "_slug": "labels", "_key": "labels", "_tooltip": "Labels (perception_msgs/LabelArray)\nAn array of labels corresponds to the batch of images.", "_message_tag": "perception_msgs/LabelArray", "_ifFastRtps": "1", "_parent": "2"}, {"mxGeometry": {"_x": "10", "_y": "-40", "_width": "200", "_height": "50", "_as": "geometry"}, "_id": "12", "_value": "", "_style": "html=1;wrap=1;whiteSpace=wrap;strokeColor=none;fontSize=14;labelBackgroundColor=transparent;align=center;fillColor=transparent;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_cell_type": "region_name", "_parent": "2"}, {"mxGeometry": {"_x": "1110", "_y": "645", "_width": "222", "_height": "172", "_as": "geometry"}, "Object": {"Object": {"_gpu_id": "1", "_memory_size": "6.1", "_number_of_cpus": "0.6", "_as": "hw_resources"}, "_batch_size": "1", "_dataset_size": "10000", "_epochs": "1", "_lr": "0.001", "_model_mode": "1", "_model_name": "my_model", "_model_path": "/MyDrive/Tensorflow_Series/Part2-Model_Implementation_and_Testing/my_model.h5", "_momentum": "0.9", "_password": "password", "_username": "username", "_number_of_instances": "1", "_region": "{\"dep_reg_id\":\"auto\",\"node_id\":null,\"burstability\":true,\"selectedResource\":\"\",\"resources_ids\":[],\"resourcesObj\":[],\"usage_factor\":0.1,\"count\":\"1\",\"isBurstableConfigurable\":false,\"spinner\":\"sent\",\"instanceType\":\"GK\",\"instanceCount\":1,\"dep_reg_resources\":{},\"isCustomerSite\":false}", "_execution_mode": "sync", "_execution_mode_interval": "1", "_is_synced": "1", "_synced_values": "{\"batch_size\":\"batch_size\"}", "_as": "options"}, "_id": "13", "_value": "", "_style": "top;fillColor=#fff;strokeColor=#929292;rotatable=0;resizable=0;", "_vertex": "1", "_connectable": "0", "_blockID": "5ef1e6833edd5aa60ad8fa2d", "_originalBlockID": "5ed4d89baf24d6d1f24770dc", "_tooltip": "A CNN classifier model as a block to be trained on the CIFAR dataset.", "_block": "1", "_yonoarc_block": "1", "_parent": "1", "_suggested": "1"}, {"mxGeometry": {"_x": "10", "_y": "15", "_width": "200", "_height": "20", "_as": "geometry"}, "_id": "14", "_value": "CIFAR CNN", "_style": "top;strokeColor=none;verticalLabelPosition=bottom;labelBackgroundColor=;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;verticalAlign=center;", "_vertex": "1", "_tooltip": "CIFAR CNN: A CNN classifier model as a block to be trained on the CIFAR dataset.", "_connectable": "0", "_parent": "13"}, {"mxGeometry": {"_y": "137", "_width": "222", "_height": "34", "_as": "geometry"}, "_id": "15", "_value": "", "_style": "shape=stencil(rVNNE4IgFPw13AkudWysTv2Jp2Iyohio2b8PQZvBr2xqhsvuvn1vGR6IBjqFkiGCC8gZoidESMOU5rK4QsiEgUYDXbKo6lVQHELBnJI6coexww8P0zPCR8N2hwYhRNlNybqIPWGQS6jSGWGQc9l0MVs3oB/3dIhSCy8rdsGLRTv+0v2+7Z/8G+Lfa4i7BrtxhzHREkccSF9A/ILtKfdexi3+UchPEbGf8MdH3GCPhNRsvoQGhp9soGUX1pYGiVRsZZ8TLoSulMwmI23biduy7jda4gU=);whiteSpace=wrap;html=1;strokeColor=#929292;fontSize=10;fillColor=#929292;fontColor=white;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_tooltip": "", "_status": "stopped", "_type": "version", "_parent": "13"}, {"mxGeometry": {"_x": "30", "_width": "40", "_height": "34", "_as": "geometry"}, "_id": "16", "_value": "<span class=\"icon-gpu\"></span>", "_style": "html=1;strokeColor=none;fontSize=16;fillColor=transparent;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;fontColor=#FFF;", "_vertex": "1", "_connectable": "0", "_parent": "15"}, {"mxGeometry": {"_x": "70", "_width": "40", "_height": "34", "_as": "geometry"}, "_id": "17", "_value": "<span class=\"icon-training\"></span>", "_style": "html=1;strokeColor=none;fontSize=16;fillColor=transparent;fontColor=#777;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_parent": "15"}, {"mxGeometry": {"_x": "110", "_width": "40", "_height": "34", "_as": "geometry"}, "_id": "18", "_value": "<span class=\"icon-error\"></span>", "_style": "html=1;strokeColor=none;fontSize=16;fillColor=transparent;fontColor=#777;movable=0;resizable=1;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_parent": "15"}, {"mxGeometry": {"_x": "150", "_width": "40", "_height": "34", "_as": "geometry"}, "_id": "19", "_value": "<span class=\"icon-duplication\"></span>", "_style": "html=1;strokeColor=none;fontSize=16;fillColor=transparent;fontColor=#777;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_parent": "15"}, {"mxGeometry": {"_x": "20", "_y": "110", "_width": "180", "_height": "20", "_as": "geometry"}, "_id": "20", "_value": "Yonohub Team", "_style": "logo;fontSize=12;strokeColor=#70707030;strokeWidth=2;arcSize=15;rounded=1;fillColor=none;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_parent": "13"}, {"mxGeometry": {"_x": "-0.015", "_y": "0.27", "_width": "6", "_height": "6", "_relative": "1", "_as": "geometry"}, "_id": "21", "_value": "<ip></ip>", "_style": "rounded=1;arcSize=50;html=1;movable=0;resizable=0;rotatable=0;deletable=0;editable=0;selectable=0;strokeColor=#2d2a53;", "_vertex": "1", "_slug": "input_batch", "_key": "input_batch", "_tooltip": "Image Batch (perception_msgs/ImageBatch)\nThe batch of images streamed from a player with a size equals to the batch size property.", "_message_tag": "perception_msgs/ImageBatch", "_ifFastRtps": "1", "_parent": "13"}, {"mxGeometry": {"_x": "-0.015", "_y": "0.5700000000000001", "_width": "6", "_height": "6", "_relative": "1", "_as": "geometry"}, "_id": "22", "_value": "<ip></ip>", "_style": "rounded=1;arcSize=50;html=1;movable=0;resizable=0;rotatable=0;deletable=0;editable=0;selectable=0;strokeColor=#2d2a53;", "_vertex": "1", "_slug": "labels", "_key": "labels", "_tooltip": "Labels (perception_msgs/LabelArray)\nAn array of labels streamed from a player with a size equals to the batch size property.", "_message_tag": "perception_msgs/LabelArray", "_ifFastRtps": "1", "_parent": "13"}, {"mxGeometry": {"_x": "0.985", "_y": "0.27", "_width": "6", "_height": "6", "_relative": "1", "_as": "geometry"}, "_id": "23", "_value": "<op></op>", "_style": "rounded=1;arcSize=50;html=1;movable=0;resizable=0;rotatable=0;deletable=0;editable=0;selectable=0;strokeColor=#2d2a53;", "_vertex": "1", "_slug": "loss", "_key": "loss", "_tooltip": "Loss (yonoarc_msgs/Float64)\nThe loss value of the training each epoch", "_message_tag": "yonoarc_msgs/Float64", "_ifFastRtps": "1", "_parent": "13"}, {"mxGeometry": {"_x": "0.985", "_y": "0.5700000000000001", "_width": "6", "_height": "6", "_relative": "1", "_as": "geometry"}, "_id": "24", "_value": "<op></op>", "_style": "rounded=1;arcSize=50;html=1;movable=0;resizable=0;rotatable=0;deletable=0;editable=0;selectable=0;strokeColor=#2d2a53;", "_vertex": "1", "_slug": "accuracy", "_key": "accuracy", "_tooltip": "Accuracy (yonoarc_msgs/Float64)\nThe accuracy percentage of the training each epoch", "_message_tag": "yonoarc_msgs/Float64", "_ifFastRtps": "1", "_parent": "13"}, {"mxGeometry": {"_x": "10", "_y": "-40", "_width": "200", "_height": "50", "_as": "geometry"}, "_id": "25", "_value": "", "_style": "html=1;wrap=1;whiteSpace=wrap;strokeColor=none;fontSize=14;labelBackgroundColor=transparent;align=center;fillColor=transparent;movable=1;resizable=0;rotatable=0;deletable=0;editable=0;connectable=0;selectable=1;locked=0;", "_vertex": "1", "_connectable": "0", "_cell_type": "region_name", "_parent": "13"}, {"mxGeometry": {"_relative": "1", "_as": "geometry"}, "_id": "26", "_style": "edgeStyle=elbowEdgeStyle;rounded=0;jettySize=auto;orthogonalLoop=1;strokeColor=#838383;strokeWidth=3;endArrow=blockThin;jumpStyle=arc;", "_edge": "1", "_parent": "1", "_source": "10", "_target": "21"}, {"mxGeometry": {"_relative": "1", "_as": "geometry"}, "_id": "27", "_style": "edgeStyle=elbowEdgeStyle;rounded=0;jettySize=auto;orthogonalLoop=1;strokeColor=#838383;strokeWidth=3;endArrow=blockThin;jumpStyle=arc;", "_edge": "1", "_parent": "1", "_source": "11", "_target": "22"}]}, "_dx": "1776", "_dy": "883", "_grid": "1", "_gridSize": "1", "_guides": "1", "_tooltips": "1", "_connect": "0", "_arrows": "1", "_fold": "1", "_page": "1", "_pageScale": "1", "_pageWidth": "1776", "_pageHeight": "1032", "_total_number_cpus": "1", "_current_number_cpus": "0", "_total_number_gpus": "1", "_current_number_gpus": "0", "_total_memory_size": "1", "_current_memory_size": "0", "_version": "5", "_background": "#FCFAFA", "_defaultParent": "1", "_scale": 0.75, "_xPos": 1318, "_yPos": 883}, "customBlocks": {}, "pipelineParameters": {"batch_size": {"type": "number", "name": "Batch Size", "deletable": true, "live_mode": false, "description": "", "nodes": ["b2", "b13"], "order": 0, "min": "1", "max": "", "value": "32"}}}